{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 386/386 [00:20<00:00, 18.71it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import h5py\n",
    "import pandas as pd\n",
    "train_h5_path = '/data1/duzhicheng/postive_NSLN/FEATURES_DIRECTORY_2_224_UNI/h5_files'\n",
    "train_all_image = []\n",
    "for i in range(len(os.listdir(train_h5_path))):\n",
    "    train_all_image.append(os.listdir(train_h5_path)[i][:-3]+ \".pt\")\n",
    "\n",
    "\n",
    "# train_all_label = '/data1/duzhicheng/postive_NSLN/label_严格删掉2个淋巴结.xlsx'  # 替换为你的Excel文件的路径\n",
    "train_all_label = '/data1/duzhicheng/postive_NSLN/label_严格包含2个淋巴结.xlsx'  # 替换为你的Excel文件的路径\n",
    "\n",
    "df = pd.read_excel(train_all_label)\n",
    "\n",
    "\n",
    "train_img_list = []\n",
    "train_label_list= []\n",
    "train_patient_list = []\n",
    "for i in range(len(train_all_image)):\n",
    "    temp = df.loc[df[\"ID\"] == train_all_image[i][:-3], \"label\"].tolist()\n",
    "    temp_patient = df.loc[df[\"ID\"] == train_all_image[i][:-3], \"patient\"].tolist()\n",
    "    if temp == []:\n",
    "        continue\n",
    "    else:\n",
    "        train_img_list.append(train_all_image[i][:-3])\n",
    "        train_label_list.append(temp[0])\n",
    "        train_patient_list.append(temp_patient[0])\n",
    "\n",
    "\n",
    "\n",
    "#制作特征\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_All_coord_list = []\n",
    "train_All_feature_list = []\n",
    "for i in tqdm(range(len(train_img_list))):\n",
    "    h5file = h5py.File(os.path.join(train_h5_path, train_img_list[i]+\".h5\"), 'r')\n",
    "    coords = h5file[\"coords\"]\n",
    "    faetures = h5file['features']\n",
    "    train_All_coord_list.append(coords[:])\n",
    "    train_All_feature_list.append(faetures[:])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#计算患者级别label\n",
    "train_patients = []\n",
    "train_patients_label = []\n",
    "train_patients_features = {}\n",
    "for i in range(len(train_patient_list)):\n",
    "    if train_patient_list[i] in train_patients:\n",
    "        continue\n",
    "    else:\n",
    "        train_patients.append(train_patient_list[i])\n",
    "        train_patients_label.append(train_label_list[i])\n",
    "        train_patients_features[train_patient_list[i]] = []\n",
    "\n",
    "for i in range(len(train_patient_list)):\n",
    "    if train_patient_list[i] in train_patients:\n",
    "        train_patients_features[train_patient_list[i]].append(train_All_feature_list[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################临床数据\n",
    "import pandas as pd\n",
    "train_patients_clinic_data = []\n",
    "train_patients_wsi_data = []\n",
    "train_patients_id = []\n",
    "# 读取CSV文件\n",
    "file_path = '/data1/duzhicheng/postive_NSLN/clinical.csv'  # 请将此处替换为你的CSV文件路径\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "for i in range(len(train_patients)):\n",
    "    # 查找第1列值为123456的行\n",
    "    target_row = df[df.iloc[:, 0] == train_patients[i]]\n",
    "\n",
    "    # 如果找到了匹配的行，返回该行的第2到第19列的值，存入列表中\n",
    "    if not target_row.empty:\n",
    "        result = target_row.iloc[0, 1:19].tolist()\n",
    "        train_patients_clinic_data.append(result)\n",
    "        train_patients_wsi_data.append(train_patients_features[train_patients[i]])\n",
    "        train_patients_id.append(train_patients[i])\n",
    "    else:\n",
    "        print(\"没有找到行\")\n",
    "\n",
    "train_patients_clinic_data = np.stack(train_patients_clinic_data, axis = 0)\n",
    "\n",
    "train_clinical_features = train_patients_clinic_data\n",
    "# wsi_features = patients_features\n",
    "# cw_features = np.hstack((patients_clinic_data, patients_features))\n",
    "\n",
    "\n",
    "# all_features = clinical_features\n",
    "train_labels = np.array(train_patients_label)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_patients_info.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "train_patients_info = {}\n",
    "train_patients_info[\"train_patients\"] = train_patients\n",
    "train_patients_info[\"train_patients_label\"] = train_patients_label\n",
    "joblib.dump(train_patients_info, \"train_patients_info.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 338/338 [00:16<00:00, 20.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139059 not founnd clinical data!\n",
      "127173 not founnd clinical data!\n",
      "153841 not founnd clinical data!\n",
      "159690 not founnd clinical data!\n",
      "155955 not founnd clinical data!\n",
      "160810 not founnd clinical data!\n",
      "166361 not founnd clinical data!\n",
      "171607 not founnd clinical data!\n",
      "174130 not founnd clinical data!\n",
      "183442 not founnd clinical data!\n",
      "184755 not founnd clinical data!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import h5py\n",
    "import pandas as pd\n",
    "test_h5_path = '/data1/duzhicheng/NSLN_shantou/FEATURES_DIRECTORY_2_224_UNI/h5_files'\n",
    "test_all_image = []\n",
    "for i in range(len(os.listdir(test_h5_path))):\n",
    "    test_all_image.append(os.listdir(test_h5_path)[i][:-3]+ \".pt\")\n",
    "\n",
    "\n",
    "test_all_label = '/data1/duzhicheng/NSLN_shantou/label.xlsx'   # 替换为你的Excel文件的路径\n",
    "\n",
    "df = pd.read_excel(test_all_label)\n",
    "\n",
    "\n",
    "test_img_list = []\n",
    "test_label_list= []\n",
    "test_patient_list = []\n",
    "for i in range(len(test_all_image)):\n",
    "    temp = df.loc[df[\"ID\"] == test_all_image[i][:-3], \"label\"].tolist()\n",
    "    temp_patient = df.loc[df[\"ID\"] == test_all_image[i][:-3], \"patient\"].tolist()\n",
    "    if temp == []:\n",
    "        continue\n",
    "    else:\n",
    "        test_img_list.append(test_all_image[i][:-3])\n",
    "        test_label_list.append(temp[0])\n",
    "        test_patient_list.append(temp_patient[0])\n",
    "\n",
    "\n",
    "\n",
    "#制作特征\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_All_coord_list = []\n",
    "test_All_feature_list = []\n",
    "for i in tqdm(range(len(test_img_list))):\n",
    "    h5file = h5py.File(os.path.join(test_h5_path, test_img_list[i]+\".h5\"), 'r')\n",
    "    coords = h5file[\"coords\"]\n",
    "    faetures = h5file['features']\n",
    "    test_All_coord_list.append(coords[:])\n",
    "    test_All_feature_list.append(faetures[:])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#计算患者级别label\n",
    "test_patients = []\n",
    "test_patients_label = []\n",
    "test_patients_features = {}\n",
    "for i in range(len(test_patient_list)):\n",
    "    if test_patient_list[i] in test_patients:\n",
    "        continue\n",
    "    else:\n",
    "        test_patients.append(test_patient_list[i])\n",
    "        test_patients_label.append(test_label_list[i])\n",
    "        test_patients_features[test_patient_list[i]] = []\n",
    "\n",
    "for i in range(len(test_patient_list)):\n",
    "    if test_patient_list[i] in test_patients:\n",
    "        test_patients_features[test_patient_list[i]].append(test_All_feature_list[i])\n",
    "\n",
    "\n",
    "\n",
    "###################临床、分类、预后数据\n",
    "import pandas as pd\n",
    "test_patients_clinic_data = []\n",
    "test_patients_wsi_data = []\n",
    "test_labels = []\n",
    "test_patients_clinic_name = []\n",
    "test_patients_dfs_time = []\n",
    "test_patients_censors = []\n",
    "\n",
    "# 读取CSV文件\n",
    "file_path = '/data1/duzhicheng/NSLN_shantou/clinical_shantou_all.xls'  # 请将此处替换为你的CSV文件路径\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "for i in range(len(test_patients)):\n",
    "    # 查找第1列值为123456的行\n",
    "    target_row = df[df.iloc[:, 0] == test_patients[i]]\n",
    "\n",
    "    # 如果找到了匹配的行，返回该行的第2到第19列的值，存入列表中\n",
    "    if not target_row.empty:\n",
    "        result = target_row.iloc[0, 1:19].tolist()\n",
    "        censor = target_row.iloc[0, 20]\n",
    "        dfs = target_row.iloc[0, 21]\n",
    "        test_patients_censors.append(censor) \n",
    "        test_patients_dfs_time.append(dfs)\n",
    "        \n",
    "        test_patients_clinic_name.append(target_row.iloc[0, 0])\n",
    "        test_patients_clinic_data.append(result)\n",
    "        test_patients_wsi_data.append(test_patients_features[test_patients[i]])\n",
    "        test_labels.append(test_patients_label[i])\n",
    "    else:\n",
    "        print(str(test_patients[i]) + \" not founnd clinical data!\")\n",
    "\n",
    "\n",
    "test_patients_clinic_data = np.stack(test_patients_clinic_data, axis = 0)\n",
    "\n",
    "test_clinical_features = test_patients_clinic_data\n",
    "# wsi_features = patients_features\n",
    "# cw_features = np.hstack((patients_clinic_data, patients_features))\n",
    "\n",
    "test_patients_dfs_time = np.stack(test_patients_dfs_time, axis = 0)\n",
    "test_patients_censors = np.stack(test_patients_censors, axis = 0)\n",
    "\n",
    "# all_features = clinical_features\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_patients_info.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "test_patients_info = {}\n",
    "test_patients_info[\"test_patients\"] = test_patients\n",
    "test_patients_info[\"test_patients_label\"] = test_patients_label\n",
    "joblib.dump(test_patients_info, \"test_patients_info.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "病理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "# 固定seed\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # cuDNN's auto-tuner    \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 数据已经存在以下变量中\n",
    "# patients_features: 字典 {患者ID: 对应患者的淋巴结特征列表，每个特征为(n, 1024)}\n",
    "# patients: 包含所有患者ID的列表\n",
    "# patients_clinic_data: 临床数据数组 (318×18)\n",
    "# labels: 患者是否患癌标签 (318×1)\n",
    "# train_index: 训练集患者序号\n",
    "# val_index: 验证集患者序号\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "class AttentionMIL(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AttentionMIL, self).__init__()\n",
    "        self.attention = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_scores = self.attention(x)  \n",
    "        attention_weights = torch.softmax(attention_scores, dim=1) \n",
    "        weighted_sum = torch.sum(attention_weights * x, dim=1) \n",
    "        return weighted_sum\n",
    "\n",
    "class pathology(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pathology, self).__init__()\n",
    "\n",
    "        # 降维层\n",
    "        self.dim_reduction = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 18),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 自注意力层\n",
    "        self.self_attention = nn.TransformerEncoderLayer(d_model=18, nhead=2, dim_feedforward=18)\n",
    "        # 分类层\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(18, 36),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(36, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, wsi_features):\n",
    "        wsi_features = wsi_features.permute(1, 0, 2)\n",
    "        wsi_features = self.dim_reduction(wsi_features)  \n",
    "        wsi_features = self.self_attention(wsi_features)\n",
    "        wsi_features, _ = torch.max(wsi_features,dim =1) \n",
    "        output_features = wsi_features\n",
    "        output = self.classifier(output_features)  # [1, 1]\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************\n",
      "seed = 85\n",
      "Epoch 0/9\n",
      "train_loss: 0.6582\n",
      "train_auc: 0.5198\n",
      "val_loss: 0.6407\n",
      "val_auc: 0.8422\n",
      "保存模型！\n",
      "Epoch 1/9\n",
      "train_loss: 0.6487\n",
      "train_auc: 0.5654\n",
      "val_loss: 0.6566\n",
      "val_auc: 0.7696\n",
      "Epoch 2/9\n",
      "train_loss: 0.6640\n",
      "train_auc: 0.4743\n",
      "val_loss: 0.6405\n",
      "val_auc: 0.7748\n",
      "Epoch 3/9\n",
      "train_loss: 0.6414\n",
      "train_auc: 0.6099\n",
      "val_loss: 0.6097\n",
      "val_auc: 0.7630\n",
      "Epoch 4/9\n",
      "train_loss: 0.6347\n",
      "train_auc: 0.6129\n",
      "val_loss: 0.6416\n",
      "val_auc: 0.6696\n",
      "Epoch 5/9\n",
      "train_loss: 0.6208\n",
      "train_auc: 0.6436\n",
      "val_loss: 0.5738\n",
      "val_auc: 0.7796\n",
      "Epoch 6/9\n",
      "train_loss: 0.5795\n",
      "train_auc: 0.6832\n",
      "val_loss: 0.5363\n",
      "val_auc: 0.8193\n",
      "Epoch 7/9\n",
      "train_loss: 0.5983\n",
      "train_auc: 0.6872\n",
      "val_loss: 0.6130\n",
      "val_auc: 0.7274\n",
      "Epoch 8/9\n",
      "train_loss: 0.6282\n",
      "train_auc: 0.5989\n",
      "val_loss: 0.6444\n",
      "val_auc: 0.7030\n",
      "Epoch 9/9\n",
      "train_loss: 0.6161\n",
      "train_auc: 0.6372\n",
      "val_loss: 0.6096\n",
      "val_auc: 0.7311\n",
      "训练过程：\n",
      "Train AUC: 0.5198369565217391\n",
      "验证过程：\n",
      "Val AUC: 0.8422222222222222\n",
      "Best Threshold: 0.37238559126853943\n",
      "Accuracy: 0.7792207792207793\n",
      "Sensitivity (Recall): 0.8888888888888888\n",
      "Specificity: 0.72\n",
      "测试过程：\n",
      "Test AUC: 0.6947557149260422\n",
      "Used Threshold: 0.37238559126853943\n",
      "Accuracy: 0.6650943396226415\n",
      "Sensitivity (Recall): 0.6907216494845361\n",
      "Specificity: 0.6434782608695652\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for seed in [85] :\n",
    "    print(\"*******************************************************\")\n",
    "    print(\"seed = {}\".format(seed))\n",
    "    \n",
    "    setup_seed(seed)\n",
    "    # train_patient_ids = np.arange(len(train_patients_label))\n",
    "    # # 使用train_test_split进行7:3划分\n",
    "    # train_index, val_index = train_test_split(train_patient_ids, test_size=0.3, stratify=train_patients_label, random_state=0)\n",
    "    # # train_index, val_index = train_index, val_index\n",
    "    train_index = np.array([206, 211, 172, 226, 210, 153,  14,  95, 212, 215, 243, 254, 216,\n",
    "       182,  71,  23, 168,  98, 107, 108, 169, 148, 131,  72,  81,  10,\n",
    "       139, 221,  99, 109,   5, 227, 159, 167,  16, 252,  11,  96, 205,\n",
    "       149, 126, 247,  91, 237,  67, 208, 174, 194, 251,  87, 155, 183,\n",
    "       158, 171,  76,  49,  61,  21, 141,  33, 121, 189,  60, 187, 137,\n",
    "        27, 143,   2,  88,  65,  26,   8,  19,  38, 144, 101,  55, 112,\n",
    "        84,  25, 249, 192,  44, 240, 220, 223,  17, 179,  92,  90, 162,\n",
    "        35, 235, 129,  20, 142,  40,   6, 242,  93,  42,  75,  53, 253,\n",
    "        54, 166,  29, 119,   4,  47,  85, 229, 193,  58,  82, 244,  77,\n",
    "       197,   0, 173, 190,  28, 178,  70, 209,  15, 186, 219, 124, 165,\n",
    "       207, 230, 185, 170, 122, 191, 225, 145, 214, 233, 203, 136, 138,\n",
    "        69, 184,  22, 132, 234,  50,  46,  12, 130,  34, 164,  30, 199,\n",
    "        18, 150,  57, 195, 105,  41, 114, 117, 232, 152, 156,  78, 113,\n",
    "       133,  79,  74,  83, 202,  31, 140,  89, 147, 146])\n",
    "\n",
    "    val_index = np.array([224, 204, 116,  62, 188, 198, 154, 103,  48,  43, 201, 111, 200,\n",
    "       177, 134,  56,  45,  39,  94, 196,  24,  64,  63, 127, 157,  80,\n",
    "       180, 181, 238,  32, 100, 104,   9,  73, 176, 255, 246, 110, 175,\n",
    "       160,  97,  86, 120, 128, 217, 228, 250,  51,   7, 245, 163, 115,\n",
    "       236, 106, 222, 118,  13, 231, 123, 151, 213, 125,  36,  37, 135,\n",
    "       239,  68, 241, 102,  52, 218,  66, 248,   3, 161,   1,  59])\n",
    "    # 训练过程\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    AttentionMIL_model = AttentionMIL(input_dim = 1024).to(device)\n",
    "    pathology_model = pathology().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(list(AttentionMIL_model.parameters()) + list(pathology_model.parameters()), lr=0.001)\n",
    "    num_epochs = 10\n",
    "    \n",
    "    train_loss_his = []\n",
    "    val_loss_his = []\n",
    "    loss_his = []\n",
    "    \n",
    "    check_point = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        AttentionMIL_model.train()\n",
    "        pathology_model.train()\n",
    "        train_label = []\n",
    "        train_prob = []\n",
    "        train_split_patients = []\n",
    "        running_loss = 0.0\n",
    "        random.shuffle(train_index)\n",
    "        \n",
    "        for index in train_index:\n",
    "            label = torch.tensor(train_labels[index], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            lymph_node_features_list = train_patients_wsi_data[index]\n",
    "            wsi_features = []\n",
    "            for i in range(len(lymph_node_features_list)):\n",
    "                wsi_features.append(AttentionMIL_model(torch.tensor(lymph_node_features_list[i], dtype=torch.float32).unsqueeze(0).to(device)))\n",
    "            wsi_features =  torch.stack(wsi_features, dim=0)\n",
    "            train_split_patients.append(train_patients[index])\n",
    "            output = pathology_model(wsi_features)\n",
    "            output.squeeze(0)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_his.append(loss.item())\n",
    "            running_loss += loss.item() \n",
    "            train_label.append(label.item())\n",
    "            train_prob.append(output.item())\n",
    "            \n",
    "        train_auc = roc_auc_score(train_label, train_prob)\n",
    "        train_loss = running_loss / len(train_index)\n",
    "        train_loss_his.append(train_loss)\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print(f'train_loss: {train_loss:.4f}')\n",
    "        print(f'train_auc: {train_auc:.4f}')\n",
    "\n",
    "        #Validation Phase\n",
    "        with torch.no_grad():\n",
    "            AttentionMIL_model.eval()\n",
    "            pathology_model.eval()\n",
    "            val_label = []\n",
    "            val_prob = []\n",
    "            val_patients = []\n",
    "            running_loss = 0.0\n",
    "            for index in val_index:\n",
    "                lymph_node_features_list = train_patients_wsi_data[index]\n",
    "                label = torch.tensor(train_labels[index], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "                val_patients.append(train_patients[index])\n",
    "                wsi_features = []\n",
    "                for i in range(len(lymph_node_features_list)):\n",
    "                    wsi_features.append(AttentionMIL_model(torch.tensor(lymph_node_features_list[i], dtype=torch.float32).unsqueeze(0).to(device)))\n",
    "                wsi_features =  torch.stack(wsi_features, dim=0)\n",
    "\n",
    "                output = pathology_model(wsi_features)\n",
    "                output.squeeze(0)\n",
    "                loss = criterion(output, label)\n",
    "                running_loss += loss.item() \n",
    "\n",
    "                val_label.append(label.item())\n",
    "                val_prob.append(output.item())\n",
    "                \n",
    "        val_auc = roc_auc_score(val_label, val_prob)\n",
    "        val_loss = running_loss / len(val_index)\n",
    "        val_loss_his.append(val_loss)\n",
    "        print(f'val_loss: {val_loss:.4f}')\n",
    "        print(f'val_auc: {val_auc:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "        if val_auc>check_point:\n",
    "            check_point = val_auc\n",
    "            print(\"保存模型！\")\n",
    "\n",
    "\n",
    "            joblib.dump(train_label, \"/data1/duzhicheng/postive_NSLN/model/train_label_pathology_{}.pkl\".format(seed))\n",
    "            joblib.dump(train_prob, \"/data1/duzhicheng/postive_NSLN/model/train_prob_pathology_{}.pkl\".format(seed))\n",
    "            joblib.dump(train_loss, \"/data1/duzhicheng/postive_NSLN/model/train_loss_pathology_{}.pkl\".format(seed))\n",
    "            joblib.dump(train_split_patients, \"/data1/duzhicheng/postive_NSLN/model/train_split_patients_{}.pkl\".format(seed))\n",
    "\n",
    "\n",
    "            joblib.dump(val_label, \"/data1/duzhicheng/postive_NSLN/model/val_label_pathology_{}.pkl\".format(seed))\n",
    "            joblib.dump(val_prob, \"/data1/duzhicheng/postive_NSLN/model/val_prob_pathology_{}.pkl\".format(seed))\n",
    "            joblib.dump(val_loss, \"/data1/duzhicheng/postive_NSLN/model/val_loss_pathology_{}.pkl\".format(seed))\n",
    "            joblib.dump(val_patients, \"/data1/duzhicheng/postive_NSLN/model/val_patients_pathology_{}.pkl\".format(seed))\n",
    "\n",
    "            torch.save(AttentionMIL_model.state_dict(), '/data1/duzhicheng/postive_NSLN/model/AttentionMIL_model_clean.pth') \n",
    "            torch.save(pathology_model.state_dict(), '/data1/duzhicheng/postive_NSLN/model/pathology_model_clean.pth') \n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "    print(\"训练过程：\")\n",
    "    train_label  = joblib.load(\"/data1/duzhicheng/postive_NSLN/model/train_label_pathology_{}.pkl\".format(seed))\n",
    "    train_prob   = joblib.load(\"/data1/duzhicheng/postive_NSLN/model/train_prob_pathology_{}.pkl\".format(seed))\n",
    "    train_split_patients   = joblib.load(\"/data1/duzhicheng/postive_NSLN/model/train_split_patients_{}.pkl\".format(seed))\n",
    "\n",
    "    y_test = train_label\n",
    "    y_prob = train_prob\n",
    "\n",
    "    # 计算ROC曲线\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # 计算AUC值\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    # 计算约登指数\n",
    "    youden_index = tpr - fpr\n",
    "    best_threshold_index = np.argmax(youden_index)\n",
    "    best_threshold = thresholds[best_threshold_index]\n",
    "\n",
    "    # 使用最佳阈值进行分类\n",
    "    y_pred_best = (y_prob >= best_threshold).astype(int)\n",
    "\n",
    "    # 计算最佳阈值下的混淆矩阵\n",
    "    cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "    # 计算准确率\n",
    "    accuracy = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "    # 计算敏感度（召回率）\n",
    "    sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "\n",
    "    # 计算特异性\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Train AUC: {auc}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"验证过程：\")\n",
    "    val_label  = joblib.load(\"/data1/duzhicheng/postive_NSLN/model/val_label_pathology_{}.pkl\".format(seed))\n",
    "    val_prob   = joblib.load(\"/data1/duzhicheng/postive_NSLN/model/val_prob_pathology_{}.pkl\".format(seed))\n",
    "    val_patients   = joblib.load(\"/data1/duzhicheng/postive_NSLN/model/val_patients_pathology_{}.pkl\".format(seed))\n",
    "\n",
    "    y_test = val_label\n",
    "    y_prob = val_prob\n",
    "\n",
    "    # 计算ROC曲线\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "\n",
    "    # 计算AUC值\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    # 计算约登指数\n",
    "    youden_index = tpr - fpr\n",
    "    best_threshold_index = np.argmax(youden_index)\n",
    "    best_threshold = thresholds[best_threshold_index]\n",
    "\n",
    "    # 使用最佳阈值进行分类\n",
    "    y_pred_best = (y_prob >= best_threshold).astype(int)\n",
    "\n",
    "    # 计算最佳阈值下的混淆矩阵\n",
    "    cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "    # 计算准确率\n",
    "    accuracy = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "    # 计算敏感度（召回率）\n",
    "    sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "\n",
    "    # 计算特异性\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Val AUC: {auc}\")\n",
    "    print(f\"Best Threshold: {best_threshold}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    \n",
    "    \n",
    "    ########################################################################################################测试阶段\n",
    "    print(\"测试过程：\")\n",
    "    \n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    AttentionMIL_model = AttentionMIL(input_dim = 1024).to(device)\n",
    "    pathology_model = pathology().to(device)\n",
    "\n",
    "    AttentionMIL_model.load_state_dict(torch.load('/data1/duzhicheng/postive_NSLN/model/AttentionMIL_model_clean.pth'))\n",
    "    pathology_model.load_state_dict(torch.load('/data1/duzhicheng/postive_NSLN/model/pathology_model_clean.pth'))\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    test_loss = []\n",
    "    check_point = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        AttentionMIL_model.eval()\n",
    "        pathology_model.eval()\n",
    "        test_label = []\n",
    "        test_prob = []\n",
    "        test_patients_name = []\n",
    "        running_loss = 0.0\n",
    "        for index in range(len(test_labels)):\n",
    "            lymph_node_features_list = test_patients_wsi_data[index]\n",
    "            \n",
    "            label = torch.tensor(test_labels[index], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "            wsi_features = []\n",
    "            for i in range(len(lymph_node_features_list)):\n",
    "                wsi_features.append(AttentionMIL_model(torch.tensor(lymph_node_features_list[i], dtype=torch.float32).unsqueeze(0).to(device)))\n",
    "            wsi_features =  torch.stack(wsi_features, dim=0)\n",
    "\n",
    "            output = pathology_model(wsi_features)\n",
    "            output.squeeze(0)\n",
    "            loss = criterion(output, label)\n",
    "        \n",
    "            running_loss += loss.item() \n",
    "\n",
    "            test_label.append(label.item())\n",
    "            test_prob.append(output.item())\n",
    "            test_patients_name.append(test_patients_clinic_name[index])\n",
    "            \n",
    "    joblib.dump(test_label, \"/data1/duzhicheng/NSLN_shantou/model/test_label_pathology_{}.pkl\".format(seed))\n",
    "    joblib.dump(test_prob, \"/data1/duzhicheng/NSLN_shantou/model/test_prob_pathology_{}.pkl\".format(seed))\n",
    "    joblib.dump(test_patients_name, \"/data1/duzhicheng/NSLN_shantou/model/test_patients_pathology_{}.pkl\".format(seed))\n",
    "    \n",
    "    auc = roc_auc_score(test_label, test_prob)\n",
    "    all_labels,all_outputs = np.array(test_label), np.array(test_prob)\n",
    "    y_pred_best = (all_outputs >= best_threshold).astype(int)\n",
    "    # 计算最佳阈值下的混淆矩阵\n",
    "    cm = confusion_matrix(all_labels, y_pred_best)\n",
    "\n",
    "    # 计算准确率\n",
    "    accuracy = accuracy_score(all_labels, y_pred_best)\n",
    "\n",
    "    # 计算敏感度（召回率）\n",
    "    sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "\n",
    "    # 计算特异性\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    print(f\"Test AUC: {auc}\")\n",
    "    print(f\"Used Threshold: {best_threshold}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 示例列表\n",
    "    \n",
    "list1 = train_split_patients\n",
    "list2 = train_label\n",
    "list3 = train_prob\n",
    "\n",
    "# 将列表转换为 DataFrame\n",
    "df = pd.DataFrame({\n",
    "    '患者ID': list1,\n",
    "    \"患者标签\": list2,\n",
    "    '预测概率': list3\n",
    "})\n",
    "\n",
    "# 保存为 Excel 文件\n",
    "df.to_excel('训练集（云肿）单病理.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 示例列表\n",
    "    \n",
    "list1 = val_patients\n",
    "list2 = val_label\n",
    "list3 = val_prob\n",
    "\n",
    "# 将列表转换为 DataFrame\n",
    "df = pd.DataFrame({\n",
    "    '患者ID': list1,\n",
    "    \"患者标签\": list2,\n",
    "    '预测概率': list3\n",
    "})\n",
    "\n",
    "# 保存为 Excel 文件\n",
    "df.to_excel('验证集（云肿）单病理.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 示例列表\n",
    "\n",
    "    \n",
    "list1 = [test_patients_name[i] for i in range(len(test_patients_name)) if i not in filter2]\n",
    "list2 = [test_label[i] for i in range(len(test_label)) if i not in filter2]\n",
    "list3 = [test_prob[i] for i in range(len(test_prob)) if i not in filter2]\n",
    "\n",
    "# 将列表转换为 DataFrame\n",
    "df = pd.DataFrame({\n",
    "    '患者ID': list1,\n",
    "    \"患者标签\": list2,\n",
    "    '预测概率': list3\n",
    "})\n",
    "\n",
    "# 保存为 Excel 文件\n",
    "df.to_excel('测试集（汕头）单病理.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
